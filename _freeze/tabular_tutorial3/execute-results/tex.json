{
  "hash": "59dcbd53329e6cc03ee396f2df9fc3d4",
  "result": {
    "engine": "knitr",
    "markdown": "# Tutorial 3: Deep neural network with explainable method\n\n# Tutorial 4: Deep neural network with explainable method for binary classification\n\n**Aims:** To predict heart disease using a deep neural network - To explain model predictions using Layer-wise Relevance Propagation (LRP) - To visualize feature importance at both individual and global levels\n\n**Data:** `heartdisease` data from `MLDataR` package.\n\n**Code description:** This code demonstrates the use of torch with custom dataset functions, training callbacks, and model explainability using the `innsight` package for binary classification with layer-wise relevance propagation (LRP).\n\n### **Packages**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nlibrary(luz)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(MLDataR)\nlibrary(innsight)\n```\n:::\n\n\n### **Data**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheart_df <- \n  heartdisease %>% \n  mutate(across(c(Sex, RestingECG, Angina), as.factor))\n```\n:::\n\n\n**Explore data.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim(heart_df)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |         |\n|:------------------------|:--------|\n|Name                     |heart_df |\n|Number of rows           |918      |\n|Number of columns        |10       |\n|_______________________  |         |\n|Column type frequency:   |         |\n|factor                   |3        |\n|numeric                  |7        |\n|________________________ |         |\n|Group variables          |None     |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                  |\n|:-------------|---------:|-------------:|:-------|--------:|:---------------------------|\n|Sex           |         0|             1|FALSE   |        2|M: 725, F: 193              |\n|RestingECG    |         0|             1|FALSE   |        3|Nor: 552, LVH: 188, ST: 178 |\n|Angina        |         0|             1|FALSE   |        2|N: 547, Y: 371              |\n\n\n**Variable type: numeric**\n\n|skim_variable    | n_missing| complete_rate|   mean|     sd|   p0|    p25|   p50|   p75|  p100|hist  |\n|:----------------|---------:|-------------:|------:|------:|----:|------:|-----:|-----:|-----:|:-----|\n|Age              |         0|             1|  53.51|   9.43| 28.0|  47.00|  54.0|  60.0|  77.0|▁▅▇▆▁ |\n|RestingBP        |         0|             1| 132.40|  18.51|  0.0| 120.00| 130.0| 140.0| 200.0|▁▁▃▇▁ |\n|Cholesterol      |         0|             1| 198.80| 109.38|  0.0| 173.25| 223.0| 267.0| 603.0|▃▇▇▁▁ |\n|FastingBS        |         0|             1|   0.23|   0.42|  0.0|   0.00|   0.0|   0.0|   1.0|▇▁▁▁▂ |\n|MaxHR            |         0|             1| 136.81|  25.46| 60.0| 120.00| 138.0| 156.0| 202.0|▁▃▇▆▂ |\n|HeartPeakReading |         0|             1|   0.89|   1.07| -2.6|   0.00|   0.6|   1.5|   6.2|▁▇▆▁▁ |\n|HeartDisease     |         0|             1|   0.55|   0.50|  0.0|   0.00|   1.0|   1.0|   1.0|▆▁▁▁▇ |\n\n\n:::\n:::\n\n\n### **Dataset function**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheart_dataset <- dataset(\n  initialize = function(df) {\n    # Pre-process and store as tensors\n    self$x_num <- df %>% \n      select(Age, RestingBP, Cholesterol, FastingBS, MaxHR, HeartPeakReading) %>% \n      mutate(across(everything(), scale)) %>% \n      as.matrix() %>% \n      torch_tensor(dtype = torch_float())\n    \n    self$x_cat <- model.matrix(~ Sex + RestingECG + Angina, data = df)[, -1] %>% \n      as.matrix() %>% \n      torch_tensor(dtype = torch_float())\n    \n    self$y <- torch_tensor(as.matrix(df$HeartDisease), dtype = torch_float())\n  },\n  .getitem = function(i) {\n    list(x = list(self$x_num[i, ], self$x_cat[i, ]), y = self$y[i])      \n  },\n  .length = function() {\n    self$y$size(1)\n  }\n)\n\n# Convert to torch dataset\nds_tensor <- heart_dataset(heart_df)\nds_tensor[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$x\n$x[[1]]\ntorch_tensor\n-1.4324\n 0.4107\n 0.8246\n-0.5510\n 1.3822\n-0.8320\n[ CPUFloatType{6} ]\n\n$x[[2]]\ntorch_tensor\n 1\n 1\n 0\n 0\n[ CPUFloatType{4} ]\n\n\n$y\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n```\n\n\n:::\n:::\n\n\n### **Split data with dataset subsets**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) \nn <- nrow(heart_df)\ntrain_size <- floor(0.6 * n)\nvalid_size <- floor(0.2 * n)\n\n# Create indices\nall_indices <- 1:n\ntrain_indices <- sample(all_indices, size = train_size)\n\nremaining_indices <- setdiff(all_indices, train_indices)\nvalid_indices <- sample(remaining_indices, size = valid_size)\n\ntest_indices <- setdiff(remaining_indices, valid_indices)\n\n# Create Subsets\ntrain_ds <- dataset_subset(ds_tensor, train_indices)\nvalid_ds <- dataset_subset(ds_tensor, valid_indices)\ntest_ds  <- dataset_subset(ds_tensor, test_indices)\n```\n:::\n\n\n### **Convert to dataloader**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_dl <- train_ds %>% \n  dataloader(batch_size = 10, shuffle = TRUE)\n\nvalid_dl <- valid_ds %>% \n  dataloader(batch_size = 10, shuffle = FALSE)\n\ntest_dl <- test_ds %>% \n  dataloader(batch_size = 10, shuffle = FALSE)\n```\n:::\n\n\n### **Specify the model**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnet <- \n  nn_module(\n    initialize = function(d_in){\n      self$net <- nn_sequential(\n        nn_linear(d_in, 32),\n        nn_relu(),\n        nn_dropout(0.5),\n        nn_linear(32, 64),\n        nn_relu(),\n        nn_dropout(0.5),\n        nn_linear(64, 1),\n        nn_sigmoid()\n      )\n    },\n    forward = function(x){\n      # x is currently a list of two tensors (numeric and categorical)\n      # Concatenate them along the feature dimension (dim=2)\n      input <- torch_cat(x, dim = 2)\n      self$net(input)\n    }\n  )\n```\n:::\n\n\n### **Fit the model**\n\n**Set parameters**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_in <- length(ds_tensor[1]$x[[1]]) + length(ds_tensor[1]$x[[2]]) # total number of features (6 numeric + 4 categorical = 10)\n```\n:::\n\n\n**Fit with callbacks**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted <- \n  net %>% \n  setup(\n    loss = nn_bce_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_binary_accuracy(), \n      luz_metric_binary_auroc()\n    )\n  ) %>% \n  set_hparams(d_in = d_in) %>% \n  fit(\n    train_dl, \n    epoch = 50, \n    valid_data = valid_dl,\n    callbacks = list(\n      luz_callback_early_stopping(patience = 10),\n      luz_callback_keep_best_model()\n    )\n  )\n```\n:::\n\n\n### **Training plot**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted %>% plot()\n```\n\n::: {.cell-output-display}\n![](tabular_tutorial3_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n**Better plot**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist <- get_metrics(fitted)\n\noptimal_epoch <- hist %>%\n  filter(metric == \"loss\", set == \"valid\") %>%\n  slice_min(value, n = 1) %>%\n  pull(epoch)\n\nhist %>%\n  ggplot(aes(x = epoch, y = value, color = set)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1.5) +\n  facet_wrap(~ metric, scales = \"free_y\", ncol = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Training vs Validation Metrics\",\n    subtitle = paste(\"Optimal epoch:\", optimal_epoch),\n    y = \"Value\",\n    x = \"Epoch\",\n    color = \"Dataset\"\n  )\n```\n\n::: {.cell-output-display}\n![](tabular_tutorial3_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### **Predict testing set**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_pred <- fitted %>% predict(test_dl)\ny_true <- ds_tensor$y[test_ds$indices] %>% as_array()\n\ndat_pred <- \n  y_pred %>% \n  as_array() %>% \n  as_data_frame() %>% \n  rename(prob = V1) %>% \n  mutate(\n    pred = factor(ifelse(prob > 0.5, 1, 0)),\n    true = factor(y_true)\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `as_data_frame()` was deprecated in tibble 2.0.0.\ni Please use `as_tibble()` (with slightly different semantics) to convert to a\n  tibble, or `as.data.frame()` to convert to a data frame.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\ni Using compatibility `.name_repair`.\ni The deprecated feature was likely used in the tibble package.\n  Please report the issue at <https://github.com/tidyverse/tibble/issues>.\n```\n\n\n:::\n\n```{.r .cell-code}\ndat_pred\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 185 x 3\n     prob pred  true \n    <dbl> <fct> <fct>\n 1 0.120  0     0    \n 2 0.286  0     0    \n 3 0.0654 0     0    \n 4 0.954  1     1    \n 5 0.0746 0     0    \n 6 0.189  0     0    \n 7 0.320  0     0    \n 8 0.324  0     0    \n 9 0.871  1     0    \n10 0.160  0     1    \n# i 175 more rows\n```\n\n\n:::\n:::\n\n\n### **Evaluate**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted %>% evaluate(test_dl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA `luz_module_evaluation`\n-- Results ---------------------------------------------------------------------\nloss: 0.4032\nacc: 0.8216\nauc: 0.8912\n```\n\n\n:::\n:::\n\n\n**Confusion matrix**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_pred %>% \n  conf_mat(true, pred) %>% \n  autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](tabular_tutorial3_files/figure-pdf/unnamed-chunk-14-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n**Accuracy**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_pred %>% \n  accuracy(truth = true, estimate = pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.822\n```\n\n\n:::\n:::\n\n\n**Plot ROC**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_pred %>% \n  roc_curve(true, prob, event_level = \"second\") %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](tabular_tutorial3_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# ROC-AUC\ndat_pred %>% \n  roc_auc(true, prob, event_level = \"second\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.893\n```\n\n\n:::\n:::\n\n\n### **Model explainability with LRP**\n\n**Prepare model for interpretation.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the sequential model\nmodel <- fitted$model$net$cpu()\n\n# Define input and output names\ninput_names <- c(\n  # Numeric variables\n  \"Age\", \"RestingBP\", \"Cholesterol\", \"FastingBS\", \"MaxHR\", \"HeartPeakReading\",\n  # Categorical dummies (from model.matrix ~ .-1)\n  \"Sex_M\", \"RestingECG_Normal\", \"RestingECG_ST\", \"Angina_Y\"\n)\n\noutput_names <- c(\"Probability of heart disease\")\n```\n:::\n\n\n**Create converter and prepare test data.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the Converter object\nconverter <- convert(\n  model,\n  input_dim = 10,\n  input_names = input_names,\n  output_names = output_names\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSkipping nn_dropout ...\nSkipping nn_dropout ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Manually extract and concatenate the test data\nidxs <- test_ds$indices\nx_num <- ds_tensor$x_num[idxs, ]\nx_cat <- ds_tensor$x_cat[idxs, ]\n\n# Combine into one tensor and convert to R array\ninput_tensor <- torch_cat(list(x_num, x_cat), dim = 2)\ninput_data <- as_array(input_tensor)\n```\n:::\n\n\n**Apply Layer-wise Relevance Propagation (LRP).**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run LRP with alpha-beta rule\nlrp_result <- run_lrp(converter, input_data, rule_name = \"alpha_beta\", rule_param = 1)\n\n# Check dimensions: Instances x Features x Outputs\ndim(get_result(lrp_result))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 185  10   1\n```\n\n\n:::\n:::\n\n\n**Individual explanations.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Individual plots for the first two test instances\nplot(lrp_result, data_idx = c(1, 2)) +\n  theme_bw() +\n  coord_flip() +\n  labs(\n    title = \"Individual Feature Relevance\",\n    subtitle = \"LRP relevance scores for Patient 1 & 2\",\n    x = \"Features\",\n    y = \"Relevance Score\"\n  )\n```\n\n::: {.cell-output-display}\n![](tabular_tutorial3_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n**Global explanations.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Global boxplot - overall feature importance across the entire test set\nboxplot(lrp_result) +\n  theme_bw() +\n  coord_flip() +\n  labs(\n    title = \"Global Feature Importance\",\n    subtitle = \"Average LRP relevance scores across all test patients\",\n    x = \"Features\",\n    y = \"Mean Relevance Score\"\n  )\n```\n\n::: {.cell-output-display}\n![](tabular_tutorial3_files/figure-pdf/unnamed-chunk-21-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n**Global explanation with raw relevance scores.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Another version of global boxplot with identity transformation\nboxplot(lrp_result, preprocess_FUN = identity) +\n  theme_bw() +\n  coord_flip() +\n  labs(\n    title = \"Global Feature Importance\",\n    subtitle = \"Distribution of raw LRP relevance scores\",\n    x = \"Features\",\n    y = \"Relevance Score\"\n  )\n```\n\n::: {.cell-output-display}\n![](tabular_tutorial3_files/figure-pdf/unnamed-chunk-22-1.pdf){fig-pos='H'}\n:::\n:::\n",
    "supporting": [
      "tabular_tutorial3_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}