[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The R torch codebook",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nWarningThis book is in its early drafting stages; many portions of the book are unfinished and all of it will go through significant revision and refinement.\n\n\n\n\n\n\nDeep learning (DL) and artificial intelligence (AI) are very popular nowadays — at least in Malaysia at the time I am writing this chapter. There are numerous open‑source and freely available resources to learn these topics, especially deep learning. The two most popular software ecosystems used in this area are Python and R. Python has been the industry standard; R, however, is catching up quickly.\nIn the Python ecosystem the two main libraries are TensorFlow and PyTorch. On the R side there are corresponding packages: tensorflow and torch. The tensorflow R package is essentially a wrapper around the Python TensorFlow API: although we write code in R, computations run in Python on the backend, and many error and warning messages therefore originate from Python. In recent years TensorFlow has reduced its GPU support. At the time of writing, macOS 12.0 (Monterey) or later (64‑bit) does not have official GPU support, and Windows (native) — Windows 7 or later (64‑bit) — also lacks GPU support for TensorFlow versions beyond 2.10. Despite these limitations, TensorFlow’s earlier establishment means there is a large selection of tutorials and books for running deep learning with TensorFlow.\nPyTorch (and the R torch package) was developed later than TensorFlow. Consequently, there are fewer tutorials and books for PyTorch and especially for torch in R. Nevertheless, a major advantage of PyTorch and torch is that GPU support is available and is often easier to set up than it used to be for TensorFlow. The relative scarcity of learning resources remains, however. At the time of writing, the most complete reference for torch is Deep Learning and Scientific Computing with R torch by Sigrid Keydana (Keydana 2023).\nThis book is my effort to compile R code related to torch — not only to consolidate my own understanding, but also to provide a practical resource for others. I hope this work will be a useful contribution.\nLastly, I want to express my gratitude and deep love to my late wife, Nurul Asmaq (al‑Fatihah); without her I am not who I am today. To my son Hanif and to my parents, Tengku Mokhtar and Nor Malaysia, I love you all more than I can express. Thank you for always supporting me.\nTengku Muhammad Hanis Bin Tengku Mokhtar, PhD\n\n\n\n\nKeydana, Sigrid. 2023. Deep Learning and Scientific Computing with r Torch. Chapman; Hall/CRC.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "About the book",
    "section": "",
    "text": "Welcome to The R torch codebook.\n\n\n\n\n\n\nWarningThis book is in its early drafting stages; many portions of the book are unfinished and all of it will go through significant revision and refinement.\n\n\n\n\n\n\nThis book is a consolidated repository of R code for deep learning using the torch ecosystem. Unlike traditional textbooks, this is not a theoretical guide. It is designed as a functional compendium for practitioners who want to move straight to implementation across diverse data modalities.\n\nCore Philosophy\nThe aim of this book is to provide a single, searchable location for all R-based torch implementations.\n\nCode-First: Text is kept to a minimum. The value lies in the executable syntax.\nModality-Driven: Chapters are organized by data type (Tabular, Image, Audio, Video), reflecting the progression of tensor dimensions and architectural complexity.\nStandardized Workflow: Every analysis follows a consistent logical flow:\n\nDataset Description: A brief overview of the data used.\nObjectives: The specific goal of the deep learning task (e.g., classification, regression).\nImplementation: The complete torch code required to load data, build the model, and execute training.\n\n\n\n\nHow to Use This Book\nThis book assumes a working knowledge of the R programming language and a basic understanding of deep learning concepts (like backpropagation and gradient descent).\nReaders are encouraged to:\n\nSearch by Modality: Use the sidebar to find the data type relevant to your project.\nAdapt the Recipes: Copy the code blocks and swap the dataset description/loader with your own local data.\nCross-Reference: Observe how torch handles different tensor dimensions as you move from Tabular (2D) to Video (5D) data.\n\n\n\nTechnical Stack\nAll examples are built using the latest versions of the following R packages:\n\ntorch: The core tensor and autograd library.\nluz: High-level API for model training and management.\ntorchvision / torchaudio: Specialized utilities for imaging and signal processing.\n\n\n\nLicense & Attribution\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nIf you use code snippets or architectures from this book in your own research, software, or publications, please cite the author as:\n\nHanis, Tengku Muhammad (2026). The R torch codebook. https://tengku-hanis.github.io/r_torch_codebook/",
    "crumbs": [
      "About the book"
    ]
  },
  {
    "objectID": "intro.html#core-philosophy",
    "href": "intro.html#core-philosophy",
    "title": "About the book",
    "section": "",
    "text": "Code-First: Text is kept to a minimum. The value lies in the executable syntax.\nModality-Driven: Chapters are organized by data type (Tabular, Image, Audio, Video), reflecting the progression of tensor dimensions and architectural complexity.\nStandardized Workflow: Every analysis follows a consistent logical flow:\n\nDataset Description: A brief overview of the data used.\nObjectives: The specific goal of the deep learning task (e.g., classification, regression).\nImplementation: The complete torch code required to load data, build the model, and execute training.",
    "crumbs": [
      "About the book"
    ]
  },
  {
    "objectID": "intro.html#how-to-use-this-book",
    "href": "intro.html#how-to-use-this-book",
    "title": "About the book",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nThis book assumes a working knowledge of the R programming language and a basic understanding of deep learning concepts (like backpropagation and gradient descent).\nReaders are encouraged to:\n\nSearch by Modality: Use the sidebar to find the data type relevant to your project.\nAdapt the Recipes: Copy the code blocks and swap the dataset description/loader with your own local data.\nCross-Reference: Observe how torch handles different tensor dimensions as you move from Tabular (2D) to Video (5D) data.",
    "crumbs": [
      "About the book"
    ]
  },
  {
    "objectID": "intro.html#technical-stack",
    "href": "intro.html#technical-stack",
    "title": "About the book",
    "section": "Technical Stack",
    "text": "Technical Stack\nAll examples are built using the latest versions of the following R packages:\n\ntorch: The core tensor and autograd library.\nluz: High-level API for model training and management.\ntorchvision / torchaudio: Specialized utilities for imaging and signal processing.",
    "crumbs": [
      "About the book"
    ]
  },
  {
    "objectID": "intro.html#license-attribution",
    "href": "intro.html#license-attribution",
    "title": "About the book",
    "section": "License & Attribution",
    "text": "License & Attribution\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nIf you use code snippets or architectures from this book in your own research, software, or publications, please cite the author as:\n\nHanis, Tengku Muhammad (2026). The R torch codebook. https://tengku-hanis.github.io/r_torch_codebook/",
    "crumbs": [
      "About the book"
    ]
  },
  {
    "objectID": "tabular_tutorial1.html",
    "href": "tabular_tutorial1.html",
    "title": "1  Tutorial 1: Basic deep neural network (binary classification)",
    "section": "",
    "text": "Aims: To predict a heart disease using a deep neural network.\nData: heartdisease data from MLDataR package.\nCode description: This codes demonstrate the use of deep neural network through torch but at the same time, still using tidymodels functions for splitting data, preprocessing, and performance metrics.\n\nPackages\n\nlibrary(torch)\nlibrary(luz)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(MLDataR)\n\n\n\nData\n\nheart_df &lt;- \nheartdisease %&gt;% \nmutate(across(c(Sex, RestingECG, Angina), as.factor))\n\nExplore data.\n\nskimr::skim(heart_df)\n\n\nData summary\n\n\nName\nheart_df\n\n\nNumber of rows\n918\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSex\n0\n1\nFALSE\n2\nM: 725, F: 193\n\n\nRestingECG\n0\n1\nFALSE\n3\nNor: 552, LVH: 188, ST: 178\n\n\nAngina\n0\n1\nFALSE\n2\nN: 547, Y: 371\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAge\n0\n1\n53.51\n9.43\n28.0\n47.00\n54.0\n60.0\n77.0\n▁▅▇▆▁\n\n\nRestingBP\n0\n1\n132.40\n18.51\n0.0\n120.00\n130.0\n140.0\n200.0\n▁▁▃▇▁\n\n\nCholesterol\n0\n1\n198.80\n109.38\n0.0\n173.25\n223.0\n267.0\n603.0\n▃▇▇▁▁\n\n\nFastingBS\n0\n1\n0.23\n0.42\n0.0\n0.00\n0.0\n0.0\n1.0\n▇▁▁▁▂\n\n\nMaxHR\n0\n1\n136.81\n25.46\n60.0\n120.00\n138.0\n156.0\n202.0\n▁▃▇▆▂\n\n\nHeartPeakReading\n0\n1\n0.89\n1.07\n-2.6\n0.00\n0.6\n1.5\n6.2\n▁▇▆▁▁\n\n\nHeartDisease\n0\n1\n0.55\n0.50\n0.0\n0.00\n1.0\n1.0\n1.0\n▆▁▁▁▇\n\n\n\n\n\n\n\nSplit data\n\nset.seed(123) \nsplit_ind &lt;- initial_validation_split(heart_df, strata = \"HeartDisease\") \nheart_train &lt;- training(split_ind) \nheart_val &lt;- validation(split_ind) \nheart_test &lt;- testing(split_ind)\n\n\n\nPreprocessing\n\nheart_rc &lt;- \nrecipe(HeartDisease ~., data = heart_train) %&gt;%\nstep_normalize(all_numeric_predictors()) %&gt;%\nstep_dummy(all_factor_predictors())\n\nheart_train_processed &lt;- heart_rc %&gt;% prep() %&gt;% bake(new_data = NULL)\n\nheart_val_processed &lt;- heart_rc %&gt;% prep() %&gt;% bake(new_data = heart_val)\n\nheart_test_processed &lt;- heart_rc %&gt;% prep() %&gt;% bake(new_data = heart_test)\n\n\n\nConver to dataloader\nConvert to torch dataset\n\ndat_train_torch &lt;- \ntensor_dataset(\n    # Features\n    heart_train_processed %&gt;% \n    select(-HeartDisease) %&gt;% \n    as.matrix() %&gt;% \n    torch_tensor(dtype = torch_float()), \n    # Outcome\n    heart_train_processed$HeartDisease %&gt;% \n    torch_tensor(dtype = torch_float()) %&gt;% \n    torch_unsqueeze(2) \n)\n\ndat_val_torch &lt;- \ntensor_dataset(\n    # Features\n    heart_val_processed %&gt;% \n    select(-HeartDisease) %&gt;% \n    as.matrix() %&gt;% \n    torch_tensor(dtype = torch_float()), \n    # Outcome\n    heart_val_processed$HeartDisease %&gt;% \n    torch_tensor(dtype = torch_float()) %&gt;% \n    torch_unsqueeze(2)\n)\n\ndat_test_torch &lt;- \ntensor_dataset( \n    # Features\n    heart_test_processed %&gt;% \n    select(-HeartDisease) %&gt;% \n    as.matrix() %&gt;% \n    torch_tensor(dtype = torch_float()),\n    # Outcome\n    heart_test_processed$HeartDisease %&gt;% \n    torch_tensor(dtype = torch_float()) %&gt;% \n    torch_unsqueeze(2) \n)\n\nDataloader\n\ntrain_dl &lt;- dataloader(dat_train_torch, batch_size = 10, shuffle = TRUE) \nval_dl &lt;- dataloader(dat_val_torch, batch_size = 10, shuffle = FALSE) \ntest_dl &lt;- dataloader(dat_test_torch, batch_size = 10, shuffle = FALSE)\n\n\n\nSpecify the model\n\nnet &lt;- nn_module( \n    initialize = function(d_in){ \n        self$net &lt;- nn_sequential(\n            nn_linear(d_in, 32),\n            nn_relu(),\n            nn_dropout(0.5),\n            nn_linear(32, 64),\n            nn_relu(),\n            nn_dropout(0.5),\n            nn_linear(64, 1),\n            nn_sigmoid()\n            )\n        },\n    forward = function(x){\n        self$net(x) \n    } \n)\n\n\n\nFit the model\nSet parameters\n\nd_in &lt;- length(heart_train_processed) - 1 # no of features minus the outcome\n\nFit\n\nfitted &lt;- \nnet %&gt;% \nsetup( \n    loss = nn_bce_loss(), \n    optimizer = optim_adam, \n    metrics = list(\n        luz_metric_binary_accuracy(), \n        luz_metric_binary_auroc()) \n) %&gt;% \nset_hparams(d_in = d_in) %&gt;% \nfit( \n    train_dl, \n    epoch = 50, \n    valid_data = val_dl \n)\n\n\n\nTraining plot\n\nfitted %&gt;% plot()\n\n\n\n\n\n\n\n\nBetter plot\n\nhist &lt;- get_metrics(fitted)\n\nhist %&gt;% \nggplot(aes(x = epoch, y = value, color = set)) + \ngeom_line(linewidth = 1) + # Draw lines \ngeom_point(size = 1.5) + # Add points for clarity \nfacet_wrap(~ metric, scales = \"free_y\", ncol = 1) + # Stack metrics vertically \ntheme_minimal() + \nlabs( \n    title = \"Training vs Validation Metrics\", \n    y = \"Value\", \n    x = \"Epoch\", \n    color = \"Dataset\" \n)\n\n\n\n\n\n\n\n\n\n\nRe-fit the model\nFit\n\nfitted2 &lt;- \nnet %&gt;% \nsetup( \n    loss = nn_bce_loss(), \n    optimizer = optim_adam, \n    metrics = list( \n        luz_metric_binary_accuracy(),\n        luz_metric_binary_auroc() ) \n) %&gt;% \nset_hparams(d_in = d_in) %&gt;% \nfit( \n    train_dl, \n    epoch = 5, \n    valid_data = val_dl \n)\n\n\n\nPredict testing set\n\ny_pred &lt;- fitted2 %&gt;% predict(test_dl)\n\ndat_pred &lt;- \ny_pred %&gt;% \nas_array() %&gt;% \nas_tibble(.name_repair = \"unique\") %&gt;% \n rename(prob = 1) %&gt;%   \nmutate(\n    pred = factor(ifelse(prob &gt; 0.5, 1, 0)),\n    true = factor(heart_test$HeartDisease)\n    )\n\nNew names:\n• `` -&gt; `...1`\n\ndat_pred\n\n# A tibble: 184 × 3\n    prob pred  true \n   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n 1 0.327 0     1    \n 2 0.411 0     0    \n 3 0.775 1     1    \n 4 0.615 1     1    \n 5 0.303 0     0    \n 6 0.258 0     0    \n 7 0.289 0     0    \n 8 0.535 1     0    \n 9 0.686 1     0    \n10 0.170 0     0    \n# ℹ 174 more rows\n\n\n\n\nEvaluate\n\nfitted %&gt;% evaluate(test_dl) # Less accurate\n\nA `luz_module_evaluation`\n── Results ─────────────────────────────────────────────────────────────────────\nloss: 0.379\nacc: 0.837\nauc: 0.9039\n\n\nConfusion matrix\n\ndat_pred %&gt;% \nconf_mat(true, pred) %&gt;% \nautoplot(\"heatmap\")\n\n\n\n\n\n\n\n\nAccuracy\n\ndat_pred %&gt;% \naccuracy(truth = true, estimate = pred)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.837\n\n\nPlot ROC\n\ndat_pred %&gt;% \nroc_curve(true, prob, event_level = \"second\") %&gt;% \nautoplot()\n\n\n\n\n\n\n\n# ROC-AUC\ndat_pred %&gt;% \nroc_auc(true, prob, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.878",
    "crumbs": [
      "Chapter 1: Deep neural network for tabular data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Tutorial 1: Basic deep neural network (binary classification)</span>"
    ]
  },
  {
    "objectID": "tabular_tutorial2.html",
    "href": "tabular_tutorial2.html",
    "title": "2  Tutorial 2: Deep neural network with dataset functions and callbacks (binary classification)",
    "section": "",
    "text": "Aims: To predict heart disease using a deep neural network with custom dataset functions and training callbacks.\nData: heartdisease data from MLDataR package.\nCode description: This code demonstrates the use of torch with custom dataset functions, dataset subsets, and training callbacks including early stopping and best model checkpointing.\n\nPackages\n\nlibrary(torch)\nlibrary(luz)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(MLDataR)\n\n\n\nData\n\nheart_df &lt;- \n  heartdisease %&gt;% \n  mutate(across(c(Sex, RestingECG, Angina), as.factor))\n\nExplore data.\n\nskimr::skim(heart_df)\n\n\nData summary\n\n\nName\nheart_df\n\n\nNumber of rows\n918\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSex\n0\n1\nFALSE\n2\nM: 725, F: 193\n\n\nRestingECG\n0\n1\nFALSE\n3\nNor: 552, LVH: 188, ST: 178\n\n\nAngina\n0\n1\nFALSE\n2\nN: 547, Y: 371\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAge\n0\n1\n53.51\n9.43\n28.0\n47.00\n54.0\n60.0\n77.0\n▁▅▇▆▁\n\n\nRestingBP\n0\n1\n132.40\n18.51\n0.0\n120.00\n130.0\n140.0\n200.0\n▁▁▃▇▁\n\n\nCholesterol\n0\n1\n198.80\n109.38\n0.0\n173.25\n223.0\n267.0\n603.0\n▃▇▇▁▁\n\n\nFastingBS\n0\n1\n0.23\n0.42\n0.0\n0.00\n0.0\n0.0\n1.0\n▇▁▁▁▂\n\n\nMaxHR\n0\n1\n136.81\n25.46\n60.0\n120.00\n138.0\n156.0\n202.0\n▁▃▇▆▂\n\n\nHeartPeakReading\n0\n1\n0.89\n1.07\n-2.6\n0.00\n0.6\n1.5\n6.2\n▁▇▆▁▁\n\n\nHeartDisease\n0\n1\n0.55\n0.50\n0.0\n0.00\n1.0\n1.0\n1.0\n▆▁▁▁▇\n\n\n\n\n\n\n\nDataset function\n\nheart_dataset &lt;- dataset(\n  initialize = function(df) {\n    # Pre-process and store as tensors\n    self$x_num &lt;- df %&gt;% \n      select(Age, RestingBP, Cholesterol, FastingBS, MaxHR, HeartPeakReading) %&gt;% \n      mutate(across(everything(), scale)) %&gt;% \n      as.matrix() %&gt;% \n      torch_tensor(dtype = torch_float())\n    \n    self$x_cat &lt;- model.matrix(~ Sex + RestingECG + Angina, data = df)[, -1] %&gt;% \n      as.matrix() %&gt;% \n      torch_tensor(dtype = torch_float())\n    \n    self$y &lt;- torch_tensor(as.matrix(df$HeartDisease), dtype = torch_float())\n  },\n  .getitem = function(i) {\n    list(x = list(self$x_num[i, ], self$x_cat[i, ]), y = self$y[i])      \n  },\n  .length = function() {\n    self$y$size(1)\n  }\n)\n\n# Convert to torch dataset\nds_tensor &lt;- heart_dataset(heart_df)\nds_tensor[1]\n\n$x\n$x[[1]]\ntorch_tensor\n-1.4324\n 0.4107\n 0.8246\n-0.5510\n 1.3822\n-0.8320\n[ CPUFloatType{6} ]\n\n$x[[2]]\ntorch_tensor\n 1\n 1\n 0\n 0\n[ CPUFloatType{4} ]\n\n\n$y\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\n\n\n\nSplit data with dataset subsets\n\nset.seed(123) \nn &lt;- nrow(heart_df)\ntrain_size &lt;- floor(0.6 * n)\nvalid_size &lt;- floor(0.2 * n)\n\n# Create indices\nall_indices &lt;- 1:n\ntrain_indices &lt;- sample(all_indices, size = train_size)\n\nremaining_indices &lt;- setdiff(all_indices, train_indices)\nvalid_indices &lt;- sample(remaining_indices, size = valid_size)\n\ntest_indices &lt;- setdiff(remaining_indices, valid_indices)\n\n# Create Subsets\ntrain_ds &lt;- dataset_subset(ds_tensor, train_indices)\nvalid_ds &lt;- dataset_subset(ds_tensor, valid_indices)\ntest_ds  &lt;- dataset_subset(ds_tensor, test_indices)\n\n\n\nConvert to dataloader\n\ntrain_dl &lt;- train_ds %&gt;% \n  dataloader(batch_size = 10, shuffle = TRUE)\n\nvalid_dl &lt;- valid_ds %&gt;% \n  dataloader(batch_size = 10, shuffle = FALSE)\n\ntest_dl &lt;- test_ds %&gt;% \n  dataloader(batch_size = 10, shuffle = FALSE)\n\n\n\nSpecify the model\n\nnet &lt;- \n  nn_module(\n    initialize = function(d_in){\n      self$net &lt;- nn_sequential(\n        nn_linear(d_in, 32),\n        nn_relu(),\n        nn_dropout(0.5),\n        nn_linear(32, 64),\n        nn_relu(),\n        nn_dropout(0.5),\n        nn_linear(64, 1),\n        nn_sigmoid()\n      )\n    },\n    forward = function(x){\n      # x is currently a list of two tensors (numeric and categorical)\n      # Concatenate them along the feature dimension (dim=2)\n      input &lt;- torch_cat(x, dim = 2)\n      self$net(input)\n    }\n  )\n\n\n\nFit the model\nSet parameters\n\nd_in &lt;- length(ds_tensor[1]$x[[1]]) + length(ds_tensor[1]$x[[2]]) # total number of features\n\nFit with callbacks\n\nfitted &lt;- \n  net %&gt;% \n  setup(\n    loss = nn_bce_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_binary_accuracy(), \n      luz_metric_binary_auroc()\n    )\n  ) %&gt;% \n  set_hparams(d_in = d_in) %&gt;% \n  fit(\n    train_dl, \n    epoch = 50, \n    valid_data = valid_dl,\n    callbacks = list(\n      luz_callback_early_stopping(patience = 10),\n      luz_callback_keep_best_model()\n    )\n  )\n\n\n\nTraining plot\n\nfitted %&gt;% plot()\n\n\n\n\n\n\n\n\nBetter plot\n\nhist &lt;- get_metrics(fitted)\n\noptimal_epoch &lt;- hist %&gt;%\n  filter(metric == \"loss\", set == \"valid\") %&gt;%\n  slice_min(value, n = 1) %&gt;%\n  pull(epoch)\n\nhist %&gt;%\n  ggplot(aes(x = epoch, y = value, color = set)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1.5) +\n  facet_wrap(~ metric, scales = \"free_y\", ncol = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Training vs Validation Metrics\",\n    subtitle = paste(\"Optimal epoch:\", optimal_epoch),\n    y = \"Value\",\n    x = \"Epoch\",\n    color = \"Dataset\"\n  )\n\n\n\n\n\n\n\n\n\n\nRe-fit the model\nNote: No need to refit manually since we use luz_callback_keep_best_model() to automatically save the best model based on validation loss.\n\n\nPredict testing set\n\ny_pred &lt;- fitted %&gt;% predict(test_dl)\ny_true &lt;- ds_tensor$y[test_ds$indices] %&gt;% as_array()\n\ndat_pred &lt;- \n  y_pred %&gt;% \n  as_array() %&gt;% \n  as_data_frame() %&gt;% \n  rename(prob = V1) %&gt;% \n  mutate(\n    pred = factor(ifelse(prob &gt; 0.5, 1, 0)),\n    true = factor(y_true)\n  )\n\nWarning: `as_data_frame()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` (with slightly different semantics) to convert to a\n  tibble, or `as.data.frame()` to convert to a data frame.\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\nℹ The deprecated feature was likely used in the tibble package.\n  Please report the issue at &lt;https://github.com/tidyverse/tibble/issues&gt;.\n\ndat_pred\n\n# A tibble: 185 × 3\n     prob pred  true \n    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n 1 0.101  0     0    \n 2 0.238  0     0    \n 3 0.0764 0     0    \n 4 0.968  1     1    \n 5 0.0822 0     0    \n 6 0.172  0     0    \n 7 0.288  0     0    \n 8 0.323  0     0    \n 9 0.846  1     0    \n10 0.128  0     1    \n# ℹ 175 more rows\n\n\n\n\nEvaluate\n\nfitted %&gt;% evaluate(test_dl)\n\nA `luz_module_evaluation`\n── Results ─────────────────────────────────────────────────────────────────────\nloss: 0.4093\nacc: 0.8378\nauc: 0.8844\n\n\nConfusion matrix\n\ndat_pred %&gt;% \n  conf_mat(true, pred) %&gt;% \n  autoplot(\"heatmap\")\n\n\n\n\n\n\n\n\nAccuracy\n\ndat_pred %&gt;% \n  accuracy(truth = true, estimate = pred)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.838\n\n\nPlot ROC\n\ndat_pred %&gt;% \n  roc_curve(true, prob, event_level = \"second\") %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n# ROC-AUC\ndat_pred %&gt;% \n  roc_auc(true, prob, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.885",
    "crumbs": [
      "Chapter 1: Deep neural network for tabular data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tutorial 2: Deep neural network with dataset functions and callbacks (binary classification)</span>"
    ]
  },
  {
    "objectID": "tabular_tutorial3.html",
    "href": "tabular_tutorial3.html",
    "title": "3  Tutorial 3: Deep neural network with explainable method (binary classification)",
    "section": "",
    "text": "Aims: To predict heart disease using a deep neural network - To explain model predictions using Layer-wise Relevance Propagation (LRP) - To visualize feature importance at both individual and global levels\nData: heartdisease data from MLDataR package.\nCode description: This code demonstrates the use of torch with custom dataset functions, training callbacks, and model explainability using the innsight package for binary classification with LRP.\n\nPackages\n\nlibrary(torch)\nlibrary(luz)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(MLDataR)\nlibrary(innsight)\n\n\n\nData\n\nheart_df &lt;- \n  heartdisease %&gt;% \n  mutate(across(c(Sex, RestingECG, Angina), as.factor))\n\nExplore data.\n\nskimr::skim(heart_df)\n\n\nData summary\n\n\nName\nheart_df\n\n\nNumber of rows\n918\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSex\n0\n1\nFALSE\n2\nM: 725, F: 193\n\n\nRestingECG\n0\n1\nFALSE\n3\nNor: 552, LVH: 188, ST: 178\n\n\nAngina\n0\n1\nFALSE\n2\nN: 547, Y: 371\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAge\n0\n1\n53.51\n9.43\n28.0\n47.00\n54.0\n60.0\n77.0\n▁▅▇▆▁\n\n\nRestingBP\n0\n1\n132.40\n18.51\n0.0\n120.00\n130.0\n140.0\n200.0\n▁▁▃▇▁\n\n\nCholesterol\n0\n1\n198.80\n109.38\n0.0\n173.25\n223.0\n267.0\n603.0\n▃▇▇▁▁\n\n\nFastingBS\n0\n1\n0.23\n0.42\n0.0\n0.00\n0.0\n0.0\n1.0\n▇▁▁▁▂\n\n\nMaxHR\n0\n1\n136.81\n25.46\n60.0\n120.00\n138.0\n156.0\n202.0\n▁▃▇▆▂\n\n\nHeartPeakReading\n0\n1\n0.89\n1.07\n-2.6\n0.00\n0.6\n1.5\n6.2\n▁▇▆▁▁\n\n\nHeartDisease\n0\n1\n0.55\n0.50\n0.0\n0.00\n1.0\n1.0\n1.0\n▆▁▁▁▇\n\n\n\n\n\n\n\nDataset function\n\nheart_dataset &lt;- dataset(\n  initialize = function(df) {\n    # Pre-process and store as tensors\n    self$x_num &lt;- df %&gt;% \n      select(Age, RestingBP, Cholesterol, FastingBS, MaxHR, HeartPeakReading) %&gt;% \n      mutate(across(everything(), scale)) %&gt;% \n      as.matrix() %&gt;% \n      torch_tensor(dtype = torch_float())\n    \n    self$x_cat &lt;- model.matrix(~ Sex + RestingECG + Angina, data = df)[, -1] %&gt;% \n      as.matrix() %&gt;% \n      torch_tensor(dtype = torch_float())\n    \n    self$y &lt;- torch_tensor(as.matrix(df$HeartDisease), dtype = torch_float())\n  },\n  .getitem = function(i) {\n    list(x = list(self$x_num[i, ], self$x_cat[i, ]), y = self$y[i])      \n  },\n  .length = function() {\n    self$y$size(1)\n  }\n)\n\n# Convert to torch dataset\nds_tensor &lt;- heart_dataset(heart_df)\nds_tensor[1]\n\n$x\n$x[[1]]\ntorch_tensor\n-1.4324\n 0.4107\n 0.8246\n-0.5510\n 1.3822\n-0.8320\n[ CPUFloatType{6} ]\n\n$x[[2]]\ntorch_tensor\n 1\n 1\n 0\n 0\n[ CPUFloatType{4} ]\n\n\n$y\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\n\n\n\nSplit data with dataset subsets\n\nset.seed(123) \nn &lt;- nrow(heart_df)\ntrain_size &lt;- floor(0.6 * n)\nvalid_size &lt;- floor(0.2 * n)\n\n# Create indices\nall_indices &lt;- 1:n\ntrain_indices &lt;- sample(all_indices, size = train_size)\n\nremaining_indices &lt;- setdiff(all_indices, train_indices)\nvalid_indices &lt;- sample(remaining_indices, size = valid_size)\n\ntest_indices &lt;- setdiff(remaining_indices, valid_indices)\n\n# Create Subsets\ntrain_ds &lt;- dataset_subset(ds_tensor, train_indices)\nvalid_ds &lt;- dataset_subset(ds_tensor, valid_indices)\ntest_ds  &lt;- dataset_subset(ds_tensor, test_indices)\n\n\n\nConvert to dataloader\n\ntrain_dl &lt;- train_ds %&gt;% \n  dataloader(batch_size = 10, shuffle = TRUE)\n\nvalid_dl &lt;- valid_ds %&gt;% \n  dataloader(batch_size = 10, shuffle = FALSE)\n\ntest_dl &lt;- test_ds %&gt;% \n  dataloader(batch_size = 10, shuffle = FALSE)\n\n\n\nSpecify the model\n\nnet &lt;- \n  nn_module(\n    initialize = function(d_in){\n      self$net &lt;- nn_sequential(\n        nn_linear(d_in, 32),\n        nn_relu(),\n        nn_dropout(0.5),\n        nn_linear(32, 64),\n        nn_relu(),\n        nn_dropout(0.5),\n        nn_linear(64, 1),\n        nn_sigmoid()\n      )\n    },\n    forward = function(x){\n      # x is currently a list of two tensors (numeric and categorical)\n      # Concatenate them along the feature dimension (dim=2)\n      input &lt;- torch_cat(x, dim = 2)\n      self$net(input)\n    }\n  )\n\n\n\nFit the model\nSet parameters\n\nd_in &lt;- length(ds_tensor[1]$x[[1]]) + length(ds_tensor[1]$x[[2]]) # total number of features (6 numeric + 4 categorical = 10)\n\nFit with callbacks\n\nfitted &lt;- \n  net %&gt;% \n  setup(\n    loss = nn_bce_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_binary_accuracy(), \n      luz_metric_binary_auroc()\n    )\n  ) %&gt;% \n  set_hparams(d_in = d_in) %&gt;% \n  fit(\n    train_dl, \n    epoch = 50, \n    valid_data = valid_dl,\n    callbacks = list(\n      luz_callback_early_stopping(patience = 10),\n      luz_callback_keep_best_model()\n    )\n  )\n\n\n\nTraining plot\n\nfitted %&gt;% plot()\n\n\n\n\n\n\n\n\nBetter plot\n\nhist &lt;- get_metrics(fitted)\n\noptimal_epoch &lt;- hist %&gt;%\n  filter(metric == \"loss\", set == \"valid\") %&gt;%\n  slice_min(value, n = 1) %&gt;%\n  pull(epoch)\n\nhist %&gt;%\n  ggplot(aes(x = epoch, y = value, color = set)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1.5) +\n  facet_wrap(~ metric, scales = \"free_y\", ncol = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Training vs Validation Metrics\",\n    subtitle = paste(\"Optimal epoch:\", optimal_epoch),\n    y = \"Value\",\n    x = \"Epoch\",\n    color = \"Dataset\"\n  )\n\n\n\n\n\n\n\n\n\n\nPredict testing set\n\ny_pred &lt;- fitted %&gt;% predict(test_dl)\ny_true &lt;- ds_tensor$y[test_ds$indices] %&gt;% as_array()\n\ndat_pred &lt;- \n  y_pred %&gt;% \n  as_array() %&gt;% \n  as_data_frame() %&gt;% \n  rename(prob = V1) %&gt;% \n  mutate(\n    pred = factor(ifelse(prob &gt; 0.5, 1, 0)),\n    true = factor(y_true)\n  )\n\nWarning: `as_data_frame()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` (with slightly different semantics) to convert to a\n  tibble, or `as.data.frame()` to convert to a data frame.\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\nℹ The deprecated feature was likely used in the tibble package.\n  Please report the issue at &lt;https://github.com/tidyverse/tibble/issues&gt;.\n\ndat_pred\n\n# A tibble: 185 × 3\n     prob pred  true \n    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n 1 0.0886 0     0    \n 2 0.326  0     0    \n 3 0.0819 0     0    \n 4 0.957  1     1    \n 5 0.0911 0     0    \n 6 0.182  0     0    \n 7 0.311  0     0    \n 8 0.383  0     0    \n 9 0.871  1     0    \n10 0.102  0     1    \n# ℹ 175 more rows\n\n\n\n\nEvaluate\n\nfitted %&gt;% evaluate(test_dl)\n\nA `luz_module_evaluation`\n── Results ─────────────────────────────────────────────────────────────────────\nloss: 0.4137\nacc: 0.8216\nauc: 0.8852\n\n\nConfusion matrix\n\ndat_pred %&gt;% \n  conf_mat(true, pred) %&gt;% \n  autoplot(\"heatmap\")\n\n\n\n\n\n\n\n\nAccuracy\n\ndat_pred %&gt;% \n  accuracy(truth = true, estimate = pred)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.822\n\n\nPlot ROC\n\ndat_pred %&gt;% \n  roc_curve(true, prob, event_level = \"second\") %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n# ROC-AUC\ndat_pred %&gt;% \n  roc_auc(true, prob, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.887\n\n\n\n\nModel explainability with LRP\nPrepare model for interpretation.\n\n# Extract the sequential model\nmodel &lt;- fitted$model$net$cpu()\n\n# Define input and output names\ninput_names &lt;- c(\n  # Numeric variables\n  \"Age\", \"RestingBP\", \"Cholesterol\", \"FastingBS\", \"MaxHR\", \"HeartPeakReading\",\n  # Categorical dummies (from model.matrix ~ .-1)\n  \"Sex_M\", \"RestingECG_Normal\", \"RestingECG_ST\", \"Angina_Y\"\n)\n\noutput_names &lt;- c(\"Probability of heart disease\")\n\nCreate converter and prepare test data.\n\n# Create the Converter object\nconverter &lt;- convert(\n  model,\n  input_dim = 10,\n  input_names = input_names,\n  output_names = output_names\n)\n\nSkipping nn_dropout ...\nSkipping nn_dropout ...\n\n# Manually extract and concatenate the test data\nidxs &lt;- test_ds$indices\nx_num &lt;- ds_tensor$x_num[idxs, ]\nx_cat &lt;- ds_tensor$x_cat[idxs, ]\n\n# Combine into one tensor and convert to R array\ninput_tensor &lt;- torch_cat(list(x_num, x_cat), dim = 2)\ninput_data &lt;- as_array(input_tensor)\n\nApply Layer-wise Relevance Propagation (LRP).\n\n# Run LRP with alpha-beta rule\nlrp_result &lt;- run_lrp(converter, input_data, rule_name = \"alpha_beta\", rule_param = 1)\n\n# Check dimensions: Instances x Features x Outputs\ndim(get_result(lrp_result))\n\n[1] 185  10   1\n\n\nIndividual explanations.\n\n# Individual plots for the first two test instances\nplot(lrp_result, data_idx = c(1, 2)) +\n  theme_bw() +\n  coord_flip() +\n  labs(\n    title = \"Individual Feature Relevance\",\n    subtitle = \"LRP relevance scores for Patient 1 & 2\",\n    x = \"Features\",\n    y = \"Relevance Score\"\n  )\n\n\n\n\n\n\n\n\nGlobal explanations.\n\n# Global boxplot - overall feature importance across the entire test set\nboxplot(lrp_result) +\n  theme_bw() +\n  coord_flip() +\n  labs(\n    title = \"Global Feature Importance\",\n    subtitle = \"Average LRP relevance scores across all test patients\",\n    x = \"Features\",\n    y = \"Mean Relevance Score\"\n  )\n\n\n\n\n\n\n\n\nGlobal explanation with raw relevance scores.\n\n# Another version of global boxplot with identity transformation\nboxplot(lrp_result, preprocess_FUN = identity) +\n  theme_bw() +\n  coord_flip() +\n  labs(\n    title = \"Global Feature Importance\",\n    subtitle = \"Distribution of raw LRP relevance scores\",\n    x = \"Features\",\n    y = \"Relevance Score\"\n  )",
    "crumbs": [
      "Chapter 1: Deep neural network for tabular data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tutorial 3: Deep neural network with explainable method (binary classification)</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Keydana, Sigrid. 2023. Deep Learning and Scientific Computing with r\nTorch. Chapman; Hall/CRC.",
    "crumbs": [
      "References"
    ]
  }
]